## Chapter 1

* The process of modeling the value of something with a set of fixed weights is called linear regression. It's one of the simplest machine learning algorithms but with machine learning the computer comes up with the weights by itself by looking up the training data.
* If we can make the cost equal zero, then our prediction algorithm is perfect for every single house.
* The total cost is the summation of the squared difference between each guess and the real value of each house. Then the whole thing is divided by the number of houses.
* Gradient descent is an iterative optimization algorithm that we can use
to find the best weights. It works by tweaking each of the weights a tiny
bit at a time in the direction that will make the cost go down.

### Minimizing the Cost Function

1. Cost function and random starting weights are passed to the gradient descent algorithm.

2. Gradient Descent iteratively tweaks the weights in an attempt to get the cost function to equal zero

3. Gradient Descent returns the weights that got the cost function as close to zero as possible: 92.1 and 10001


### Review

1. Create an equation to model the problem

2. Create a cost function to quantify the error in the model;

3. Use an optimizat ion algorithm (like gradient decent) to find the model parameters that minimize the cost function
